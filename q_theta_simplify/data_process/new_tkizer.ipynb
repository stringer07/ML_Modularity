{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_cache=False\n",
    "\n",
    "model_path=Path('./models')\n",
    "pretrained_mdl= \"t5-small\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading t5-small model...\n",
      "32104\n"
     ]
    }
   ],
   "source": [
    "if pretrained_mdl == \"t5-small\" :\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path / \"t5-small-new\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path / \"t5-small-new\")\n",
    "    print(\"loading t5-small model...\")\n",
    "\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape before resizing: torch.Size([32104, 512])\n",
      "32104\n",
      "32104\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding matrix shape before resizing:\", model.shared.weight.shape)\n",
    "\n",
    "new_token1=[\"q_theta\", \"pow\", \"INT+\", \"INT-\", \"add\", \"mul\", \"z\", \"t\"]\n",
    "\n",
    "print(len(tokenizer))\n",
    "tokenizer.add_tokens(new_token1)\n",
    "print(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32104, 512])\n"
     ]
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.shared.weight.shape)\n",
    "#print(\"Embedding matrix shape after resizing:\", model.shared.weight[-len(new_token1):].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs to zero: [32100, 32101, 32102, 32103, 13039, 4115, 172, 17]\n"
     ]
    }
   ],
   "source": [
    "token_ids_to_zero = tokenizer.convert_tokens_to_ids(new_token1)\n",
    "print(f\"Token IDs to zero: {token_ids_to_zero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现有embedding统计: mean=0.1213, std=23.1957\n",
      "Token IDs to initialize: [32100, 32101, 32102, 32103, 13039, 4115, 172, 17]\n",
      "\n",
      "New token embeddings after initialization:\n",
      "  q_theta (id=32100): mean=-0.0308, std=0.4735\n",
      "  pow (id=32101): mean=-0.0232, std=0.4466\n",
      "  INT+ (id=32102): mean=-0.0183, std=0.4596\n",
      "  INT- (id=32103): mean=-0.0049, std=0.4631\n",
      "  add (id=13039): mean=-0.0067, std=0.4885\n",
      "  mul (id=4115): mean=0.0220, std=0.4522\n",
      "  z (id=172): mean=-0.0307, std=0.4692\n",
      "  t (id=17): mean=0.0102, std=0.4832\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.shared.weight\n",
    "\n",
    "embed_mean = embeddings.mean().item()\n",
    "embed_std = embeddings.std().item()\n",
    "\n",
    "print(f\"现有embedding统计: mean={embed_mean:.4f}, std={embed_std:.4f}\")\n",
    "print(f\"Token IDs to initialize: {token_ids_to_zero}\")\n",
    "\n",
    "print(\"\\nNew token embeddings after initialization:\")\n",
    "for token, tid in zip(new_token1, token_ids_to_zero):\n",
    "    emb = embeddings[tid]\n",
    "    print(f\"  {token} (id={tid}): mean={emb.mean().item():.4f}, std={emb.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/t5-small-new/tokenizer_config.json',\n",
       " './models/t5-small-new/special_tokens_map.json',\n",
       " './models/t5-small-new/spiece.model',\n",
       " './models/t5-small-new/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/t5-small-new\")\n",
    "tokenizer.save_pretrained(\"./models/t5-small-new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32100,  4115, 13039,   172,    17,  8947,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"q_theta\",\"mul\",\"add\",\"z\",\"t\",\"apple\"], return_tensors=\"pt\",is_split_into_words=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
