{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_cache=False\n",
    "\n",
    "model_path=Path('./models')\n",
    "\n",
    "pretrained_mdl= \"t5-small\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading t5-small model...\n"
     ]
    }
   ],
   "source": [
    "if pretrained_mdl == \"t5-small\" :\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path / \"t5-small-new\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path / \"t5-small-new\")\n",
    "    print(\"loading t5-small model...\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()[1:-1]\n",
    "        tokens = line.split(', ')\n",
    "        tokens = [token.strip(\"'\") for token in tokens]\n",
    "        data.append(tokens)\n",
    "    return data\n",
    "\n",
    "#preprocess data\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['input_ids'], is_split_into_words=True, padding=False, truncation=False)\n",
    "    targets = tokenizer(examples['labels'], is_split_into_words=True, padding=False, truncation=False)\n",
    "    \n",
    "    model_inputs = {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': targets['input_ids']\n",
    "    }\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading origin data ...\n",
      "loading simple data ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db97a4ef1bc04468b75a7dbdbce6d73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c008f90a0e4017bc9776d252329760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d08a9b313f4e498948c8a4b1b7bd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/450000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fe15b6400746658270c64475f387e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed data save to disk sucessfully! \n"
     ]
    }
   ],
   "source": [
    "print(\"loading origin data ...\")\n",
    "train_origin_prefix = load_data('./data/train/random_ns_nt/prefix_origin.txt')\n",
    "print(\"loading simple data ...\")\n",
    "train_simple_prefix = load_data('./data/train/random_ns_nt/prefix_simple.txt')\n",
    "assert len(train_origin_prefix) == len(train_simple_prefix)\n",
    "# split dataset for train and test\n",
    "train_origin_prefix, eval_origin_prefix, train_simple_prefix, eval_simple_prefix = train_test_split(\n",
    "    train_origin_prefix, train_simple_prefix, test_size=0.1, random_state=42\n",
    ")\n",
    "# Create dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_origin_prefix,\n",
    "    'labels': train_simple_prefix\n",
    "})\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    'input_ids': eval_origin_prefix,\n",
    "    'labels': eval_simple_prefix\n",
    "})\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'eval': eval_dataset\n",
    "})\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input_ids\", \"labels\"])\n",
    "\n",
    "tokenized_dataset.save_to_disk(\"./data/train/preprocessed_data\")\n",
    "print(\"preprocessed data save to disk sucessfully! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4115, 32101, 32100, 4115, 32103, 209, 172, 13039, 32103, 305, 17, 32103, 209, 4115, 32101, 32100, 4115, 172, 32101, 13039, 32102, 220, 4115, 32102, 314, 17, 32103, 209, 4115, 32101, 13039, 32102, 220, 4115, 32102, 314, 17, 32103, 209, 13039, 32103, 209, 4115, 32103, 209, 17, 32103, 209, 4115, 32101, 32100, 4115, 32103, 209, 4115, 172, 32101, 13039, 32103, 431, 4115, 32102, 305, 17, 32103, 209, 4115, 32101, 13039, 32103, 431, 4115, 32102, 305, 17, 32103, 209, 13039, 32103, 209, 489, 4115, 32102, 209, 314, 17, 32103, 209, 4115, 32100, 4115, 32103, 209, 172, 13039, 32103, 431, 17, 32100, 4115, 172, 32101, 13039, 32102, 220, 4115, 32102, 314, 17, 32103, 209, 4115, 32101, 13039, 32102, 220, 4115, 32102, 314, 17, 32103, 209, 13039, 32102, 505, 4115, 32102, 209, 209, 17, 1]\n",
      "[4115, 32100, 4115, 172, 32101, 13039, 32103, 431, 4115, 32102, 489, 17, 32103, 209, 4115, 32101, 13039, 32103, 431, 4115, 32102, 489, 17, 32103, 209, 13039, 32102, 305, 4115, 32103, 431, 17, 32100, 4115, 32103, 209, 4115, 172, 32101, 13039, 32102, 505, 17, 32103, 209, 4115, 32101, 13039, 32102, 505, 17, 32103, 209, 13039, 32103, 668, 4115, 32103, 209, 17, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][100]['input_ids']) \n",
    "print(tokenized_dataset['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "统计非0长度:   0%|          | 0/450000 [00:00<?, ?样本/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "统计非0长度: 100%|██████████| 450000/450000 [00:33<00:00, 13374.49样本/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非0长度最大值: 347\n",
      "非0长度最小值: 4\n",
      "非0长度分布:\n",
      "[(169, 3886), (172, 3823), (166, 3724), (170, 3508), (171, 3495), (175, 3472), (163, 3407), (173, 3395), (174, 3379), (167, 3354), (105, 3296), (102, 3282), (168, 3250), (176, 3250), (177, 3142), (99, 3073), (160, 3033), (178, 3022), (164, 3013), (165, 2982)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "nonzero_lens = [sum(1 for t in x['input_ids'] if t != 0) for x in tqdm(tokenized_dataset['train'], desc='统计非0长度', unit='样本')]\n",
    "print(f\"非0长度最大值: {max(nonzero_lens)}\")\n",
    "print(f\"非0长度最小值: {min(nonzero_lens)}\")\n",
    "print(f\"非0长度分布:\")\n",
    "print(Counter(nonzero_lens).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "数据异常检查\n",
      "==================================================\n",
      "实际词表大小: 32104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "检查异常样本:   0%|          | 0/450000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "检查异常样本: 100%|██████████| 450000/450000 [01:46<00:00, 4240.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "检查结果:\n",
      "  ✅ empty_input: 0 个异常\n",
      "  ✅ empty_label: 0 个异常\n",
      "  ✅ invalid_token_input: 0 个异常\n",
      "  ✅ invalid_token_label: 0 个异常\n",
      "  ✅ very_short_input: 0 个异常\n",
      "  ✅ very_short_label: 0 个异常\n",
      "\n",
      "✅ 数据检查通过，未发现异常!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"数据异常检查\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 使用 len(tokenizer) 而非 vocab_size，因为有新增token\n",
    "actual_vocab_size = len(tokenizer)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"实际词表大小: {actual_vocab_size}\")\n",
    "\n",
    "anomalies = {\n",
    "    'empty_input': [],      # input_ids 全是padding\n",
    "    'empty_label': [],      # labels 全是padding\n",
    "    'invalid_token_input': [],  # input中有超出词表的token\n",
    "    'invalid_token_label': [],  # label中有超出词表的token\n",
    "    'very_short_input': [],     # input非常短 (< 3个有效token)\n",
    "    'very_short_label': [],     # label非常短\n",
    "}\n",
    "\n",
    "for i, sample in enumerate(tqdm(tokenized_dataset['train'], desc='检查异常样本')):\n",
    "    input_ids = sample['input_ids']\n",
    "    labels = sample['labels']\n",
    "    \n",
    "    # 计算有效token数量（非padding）\n",
    "    input_valid = sum(1 for t in input_ids if t != pad_token_id)\n",
    "    label_valid = sum(1 for t in labels if t != pad_token_id)\n",
    "    \n",
    "    # 检查空序列\n",
    "    if input_valid == 0:\n",
    "        anomalies['empty_input'].append(i)\n",
    "    if label_valid == 0:\n",
    "        anomalies['empty_label'].append(i)\n",
    "    \n",
    "    # 检查非法token - 使用实际词表大小\n",
    "    if any(t < 0 or t >= actual_vocab_size for t in input_ids):\n",
    "        anomalies['invalid_token_input'].append(i)\n",
    "    if any(t < 0 or t >= actual_vocab_size for t in labels if t != -100):\n",
    "        anomalies['invalid_token_label'].append(i)\n",
    "    \n",
    "    # 检查过短序列\n",
    "    if 0 < input_valid < 3:\n",
    "        anomalies['very_short_input'].append(i)\n",
    "    if 0 < label_valid < 3:\n",
    "        anomalies['very_short_label'].append(i)\n",
    "\n",
    "print(\"\\n检查结果:\")\n",
    "for key, indices in anomalies.items():\n",
    "    count = len(indices)\n",
    "    if count > 0:\n",
    "        print(f\"  ❌ {key}: {count} 个样本\")\n",
    "        print(f\"     前5个索引: {indices[:5]}\")\n",
    "    else:\n",
    "        print(f\"  ✅ {key}: 0 个异常\")\n",
    "\n",
    "total_anomalies = sum(len(v) for v in anomalies.values())\n",
    "if total_anomalies > 0:\n",
    "    print(f\"\\n⚠️  共发现 {total_anomalies} 处异常!\")\n",
    "else:\n",
    "    print(f\"\\n✅ 数据检查通过，未发现异常!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
